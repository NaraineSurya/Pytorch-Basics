{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9URsJ+W8k176lzvc28LEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaraineSurya/Pytorch-Basics/blob/master/Auto%20Grad%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g1BKB5ByH4hg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0iSkNJ-_ctD2"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ozNC0HqdezF",
        "outputId": "c9593c5a-9d1f-4b80-d274-e12e52d6bd71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensors with no requires gradient\n",
        "x = torch.randn(3, requires_grad = False)\n",
        "print(x)\n",
        "\n",
        "y= x + 2\n",
        "print(y)\n",
        "\n",
        "z = y * y * 2\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9ARQi9mdhwL",
        "outputId": "3bf98b3f-13f3-4ae4-8385-48a2b20bc05d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5327, -0.0997,  0.1249])\n",
            "tensor([2.5327, 1.9003, 2.1249])\n",
            "tensor([12.8292,  7.2222,  9.0306])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradients can be calculated only for scalar outputs**"
      ],
      "metadata": {
        "id": "2lvX8EeYkoEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensors with requires_ grad = True\n",
        "x = torch.randn(3, requires_grad = True)\n",
        "print(x)\n",
        "\n",
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = y * y * 2\n",
        "print(z)\n",
        "\n",
        "# Calculating the gradients with the scalar value (mean)\n",
        "z = z.mean()\n",
        "print(z)\n",
        "\n",
        "z.backward() # This line is used to calculate the gradients in the network dz/dx\n",
        "print(x.grad) # This will show the gradient value of x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt8_v5Uhe7Mw",
        "outputId": "4329d687-3e32-4a77-c23c-9fdb80baeb89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.6474,  1.0160,  0.1497], requires_grad=True)\n",
            "tensor([1.3526, 3.0160, 2.1497], grad_fn=<AddBackward0>)\n",
            "tensor([ 3.6590, 18.1920,  9.2427], grad_fn=<MulBackward0>)\n",
            "tensor(10.3646, grad_fn=<MeanBackward0>)\n",
            "tensor([1.8035, 4.0213, 2.8663])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculating Gradient for a vector**"
      ],
      "metadata": {
        "id": "sQNdELbVkp8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating gradients with the vectors\n",
        "x = torch.randn(3, requires_grad = True)\n",
        "print(x)\n",
        "\n",
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = y * y * 2\n",
        "print(z)\n",
        "\n",
        "z.backward() # This line will produce error because gradients can calculated only for scalar values not for vectors, matrices or tensors\n",
        "# If you need to clarify this use this given link why it needs to be scalar ??\n",
        "# https://discuss.pytorch.org/t/could-loss-function-output-a-vector-instead-of-a-scalar/122084\n",
        "# https://pytorch.org/docs/main/generated/torch.autograd.backward.html#torch.autograd.backward\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AoXwif8ifHdN",
        "outputId": "096cecec-ffa8-4f53-85ad-c8534143bcac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5236,  0.2007, -0.0063], requires_grad=True)\n",
            "tensor([2.5236, 2.2007, 1.9937], grad_fn=<AddBackward0>)\n",
            "tensor([12.7372,  9.6865,  7.9497], grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-372fca519344>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line will produce error because gradients can calculated only for scalar values not for vectors, matrices or tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# If you need to clarify this use this given link why it needs to be scalar ??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# https://discuss.pytorch.org/t/could-loss-function-output-a-vector-instead-of-a-scalar/122084\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mout_numel_is_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout_numel_is_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    199\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculating Gradient with a vector using Jacobian vector product**"
      ],
      "metadata": {
        "id": "DiUKTTGx_dzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use vector jacobian product to get the gradient using the vector v\n",
        "x = torch.randn(3, requires_grad = True)\n",
        "print(x)\n",
        "\n",
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = y * y * 2\n",
        "print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 0.11, 0.001], dtype=torch.float32)\n",
        "z.backward(v) # Now we can calculate using the vector jacobian product to calculate the gradient\n",
        "print(x.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAEsAJxXjw0w",
        "outputId": "19b36e66-7e0f-4536-de75-f2c5497cf32b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.8866,  1.3890, -1.0386], requires_grad=True)\n",
            "tensor([1.1134, 3.3890, 0.9614], grad_fn=<AddBackward0>)\n",
            "tensor([ 2.4793, 22.9702,  1.8485], grad_fn=<MulBackward0>)\n",
            "tensor([0.4454, 1.4911, 0.0038])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Requires_Grad False**"
      ],
      "metadata": {
        "id": "kw0yIFjsGb8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "x.requires_grad_(False) #Inplace Making of gradients to false\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHsmG4LQ_TOO",
        "outputId": "4bea2c48-d3d5-493e-d92b-16db494215e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0757,  0.0392, -1.2966], requires_grad=True)\n",
            "tensor([-0.0757,  0.0392, -1.2966])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4, requires_grad=True)\n",
        "# print(x)\n",
        "y = x.detach() #This will detach the tensor from the gradients being True\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv60JfPAGvpY",
        "outputId": "c1a3785a-8d13-4aa5-ad20-5a3d47278958"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2615, -1.7548, -0.4787, -0.5128], requires_grad=True)\n",
            "tensor([-0.2615, -1.7548, -0.4787, -0.5128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(6, requires_grad=True)\n",
        "y = torch.ones(6, requires_grad=True)\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "with torch.no_grad():\n",
        "  z = x + y\n",
        "  print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpO631yJHFY1",
        "outputId": "d7148476-a759-4f7e-9ad1-88b990be4673"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.5608,  0.3635,  0.7285, -1.9027, -0.6407, -0.1502],\n",
            "       requires_grad=True)\n",
            "tensor([1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
            "tensor([ 2.5608,  1.3635,  1.7285, -0.9027,  0.3593,  0.8498])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDN-t-DvHaYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}