{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD/P3WczujeSru7IPNEfl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaraineSurya/Pytorch-Basics/blob/master/BackPropagation_Torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zLmsSLTeaP-n"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculate Loss and Gradients For a Forward Pass**"
      ],
      "metadata": {
        "id": "6kUqWikBaUyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "y_pred = w * x\n",
        "loss = (y_pred - y) ** 2\n",
        "print(loss)\n",
        "\n",
        "loss.backward()\n",
        "# dloss/dw = 2(y_pred - y)*w\n",
        "# dloss/dw = 2(-1)* 1 = -2\n",
        "# w.grad = dloss/dw = -2\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3sqc0bGaUH8",
        "outputId": "ff9f7189-9030-45c5-f232-c55c20287c4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manual Forward Pass and Back Propagation**"
      ],
      "metadata": {
        "id": "N2IzicpAfFgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialiaze Input and Output\n",
        "x = np.array([1,2,3,4], dtype='float32')\n",
        "y = np.array([2,4,6,8], dtype='float32')\n",
        "\n",
        "# Initiailize Weights\n",
        "w = 0.0\n",
        "\n",
        "# f(x) = w * x\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# MSE Loss = 1/n * (y_hat - y) ** 2\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "# dw = 1/n * 2w * (y_hat - y)\n",
        "def gradients(y, y_pred, x):\n",
        "  return np.dot(2*x, y_pred- y).mean()\n",
        "\n",
        "print(f\"Before Training f(5) = {forward(5)}\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  dw = gradients(y, y_pred, x)\n",
        "\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f\"Epoch : {epoch+1} - w : {w:.4f} - loss : {l:.8f}\")\n",
        "\n",
        "print(f\"After Training f(5) = {forward(5)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuKnX9WXawdg",
        "outputId": "b2b7dff8-72c0-43ce-a02a-b09cc3e0a2b2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training f(5) = 0.0\n",
            "Epoch : 1 - w : 1.2000 - loss : 30.00000000\n",
            "Epoch : 3 - w : 1.8720 - loss : 0.76800019\n",
            "Epoch : 5 - w : 1.9795 - loss : 0.01966083\n",
            "Epoch : 7 - w : 1.9967 - loss : 0.00050331\n",
            "Epoch : 9 - w : 1.9995 - loss : 0.00001288\n",
            "Epoch : 11 - w : 1.9999 - loss : 0.00000033\n",
            "Epoch : 13 - w : 2.0000 - loss : 0.00000001\n",
            "Epoch : 15 - w : 2.0000 - loss : 0.00000000\n",
            "Epoch : 17 - w : 2.0000 - loss : 0.00000000\n",
            "Epoch : 19 - w : 2.0000 - loss : 0.00000000\n",
            "After Training f(5) = 9.999999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manual Forward Pass and BackPropagation Using Autograd**"
      ],
      "metadata": {
        "id": "gJmLUioKlRXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0, requires_grad = True, dtype=torch.float32)\n",
        "\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "def loss(y , y_pred):\n",
        "  return ((y_pred- y)**2).mean()\n",
        "\n",
        "print(f\"Before Training f(5) : {forward(5)}\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  l.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  w.grad.zero_()\n",
        "  if epoch % 2 ==0:\n",
        "    print(f\"Epoch : {epoch+1} : w = {w:.4f} : loss = {l:.8f} \")\n",
        "\n",
        "print(f\"After Training f(5) : {forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5LeYtpdfWsg",
        "outputId": "799e16fd-26cc-43c4-aca1-6e980faba500"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training f(5) : 0.0\n",
            "Epoch : 1 : w = 0.3000 : loss = 30.00000000 \n",
            "Epoch : 3 : w = 0.7717 : loss = 15.66018772 \n",
            "Epoch : 5 : w = 1.1126 : loss = 8.17471695 \n",
            "Epoch : 7 : w = 1.3588 : loss = 4.26725292 \n",
            "Epoch : 9 : w = 1.5368 : loss = 2.22753215 \n",
            "Epoch : 11 : w = 1.6653 : loss = 1.16278565 \n",
            "Epoch : 13 : w = 1.7582 : loss = 0.60698116 \n",
            "Epoch : 15 : w = 1.8253 : loss = 0.31684780 \n",
            "Epoch : 17 : w = 1.8738 : loss = 0.16539653 \n",
            "Epoch : 19 : w = 1.9088 : loss = 0.08633806 \n",
            "After Training f(5) : 9.612405776977539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manual Forward Pass**"
      ],
      "metadata": {
        "id": "YOOqL15Zs_xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0, requires_grad = True, dtype=torch.float32)\n",
        "\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "print(f\"Before Training f(5) : {forward(5)}\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 30\n",
        "\n",
        "optim = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Forward Pass\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  # Calculate Losses\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  # Backward Pass - Gradients\n",
        "  l.backward()\n",
        "\n",
        "  # Update Parameters\n",
        "  optim.step()\n",
        "\n",
        "  # Zero Gradients\n",
        "  optim.zero_grad()\n",
        "\n",
        "  if epoch % 2 ==0:\n",
        "    print(f\"Epoch : {epoch+1} : w = {w:.4f} : loss = {l:.8f} \")\n",
        "\n",
        "print(f\"After Training f(5) : {forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccsL7zSCmd_E",
        "outputId": "a6362ce6-341a-48a1-8dd4-7c3adf89b2da"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training f(5) : 0.0\n",
            "Epoch : 1 : w = 0.3000 : loss = 30.00000000 \n",
            "Epoch : 3 : w = 0.7717 : loss = 15.66018772 \n",
            "Epoch : 5 : w = 1.1126 : loss = 8.17471695 \n",
            "Epoch : 7 : w = 1.3588 : loss = 4.26725292 \n",
            "Epoch : 9 : w = 1.5368 : loss = 2.22753215 \n",
            "Epoch : 11 : w = 1.6653 : loss = 1.16278565 \n",
            "Epoch : 13 : w = 1.7582 : loss = 0.60698116 \n",
            "Epoch : 15 : w = 1.8253 : loss = 0.31684780 \n",
            "Epoch : 17 : w = 1.8738 : loss = 0.16539653 \n",
            "Epoch : 19 : w = 1.9088 : loss = 0.08633806 \n",
            "Epoch : 21 : w = 1.9341 : loss = 0.04506890 \n",
            "Epoch : 23 : w = 1.9524 : loss = 0.02352631 \n",
            "Epoch : 25 : w = 1.9656 : loss = 0.01228084 \n",
            "Epoch : 27 : w = 1.9751 : loss = 0.00641066 \n",
            "Epoch : 29 : w = 1.9820 : loss = 0.00334642 \n",
            "After Training f(5) : 9.92369270324707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pytorch Implementation of Forward and Backward**"
      ],
      "metadata": {
        "id": "N54XpXR_xqfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "# model = nn.Linear(n_features, n_features)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    self.Lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.Lin(x)\n",
        "\n",
        "model = LinearRegression(n_features, n_features)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "x_test = torch.tensor([5], dtype=torch.float32)\n",
        "print(f\"Before Training f(5) : {model(x_test)}\")\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "\n",
        "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Forward Pass\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Calculate Losses\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  # Backward Pass - Gradients\n",
        "  l.backward()\n",
        "\n",
        "  # Update Parameters\n",
        "  optim.step()\n",
        "\n",
        "  # Zero Gradients\n",
        "  optim.zero_grad()\n",
        "\n",
        "  if epoch % 2 ==0:\n",
        "    [w, b] = model.parameters()\n",
        "    # print(w, b)\n",
        "    print(f\"Epoch : {epoch+1} : w = {w[0, 0]:.4f} : loss = {l:.6f} \")\n",
        "\n",
        "print(f\"After Training f(5) : {model(x_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdZqmLOktzZN",
        "outputId": "cc10df56-970c-4f6d-df8f-c7e0f710be98"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Before Training f(5) : tensor([-3.5914], grad_fn=<ViewBackward0>)\n",
            "Epoch : 1 : w = 3.1306 : loss = 52.409943 \n",
            "Epoch : 3 : w = 2.2843 : loss = 10.781160 \n",
            "Epoch : 5 : w = 1.9175 : loss = 2.364693 \n",
            "Epoch : 7 : w = 1.7652 : loss = 0.646154 \n",
            "Epoch : 9 : w = 1.7085 : loss = 0.280321 \n",
            "Epoch : 11 : w = 1.6940 : loss = 0.189395 \n",
            "Epoch : 13 : w = 1.6978 : loss = 0.155852 \n",
            "Epoch : 15 : w = 1.7093 : loss = 0.135618 \n",
            "Epoch : 17 : w = 1.7236 : loss = 0.119610 \n",
            "Epoch : 19 : w = 1.7386 : loss = 0.105820 \n",
            "Epoch : 21 : w = 1.7534 : loss = 0.093685 \n",
            "Epoch : 23 : w = 1.7677 : loss = 0.082956 \n",
            "Epoch : 25 : w = 1.7813 : loss = 0.073458 \n",
            "Epoch : 27 : w = 1.7941 : loss = 0.065048 \n",
            "Epoch : 29 : w = 1.8063 : loss = 0.057601 \n",
            "Epoch : 31 : w = 1.8177 : loss = 0.051006 \n",
            "Epoch : 33 : w = 1.8284 : loss = 0.045167 \n",
            "Epoch : 35 : w = 1.8385 : loss = 0.039996 \n",
            "Epoch : 37 : w = 1.8481 : loss = 0.035417 \n",
            "Epoch : 39 : w = 1.8570 : loss = 0.031362 \n",
            "Epoch : 41 : w = 1.8655 : loss = 0.027772 \n",
            "Epoch : 43 : w = 1.8734 : loss = 0.024592 \n",
            "Epoch : 45 : w = 1.8809 : loss = 0.021777 \n",
            "Epoch : 47 : w = 1.8879 : loss = 0.019284 \n",
            "Epoch : 49 : w = 1.8945 : loss = 0.017076 \n",
            "Epoch : 51 : w = 1.9007 : loss = 0.015121 \n",
            "Epoch : 53 : w = 1.9066 : loss = 0.013390 \n",
            "Epoch : 55 : w = 1.9121 : loss = 0.011857 \n",
            "Epoch : 57 : w = 1.9173 : loss = 0.010500 \n",
            "Epoch : 59 : w = 1.9222 : loss = 0.009298 \n",
            "Epoch : 61 : w = 1.9267 : loss = 0.008233 \n",
            "Epoch : 63 : w = 1.9311 : loss = 0.007291 \n",
            "Epoch : 65 : w = 1.9351 : loss = 0.006456 \n",
            "Epoch : 67 : w = 1.9390 : loss = 0.005717 \n",
            "Epoch : 69 : w = 1.9426 : loss = 0.005062 \n",
            "Epoch : 71 : w = 1.9459 : loss = 0.004483 \n",
            "Epoch : 73 : w = 1.9491 : loss = 0.003970 \n",
            "Epoch : 75 : w = 1.9521 : loss = 0.003515 \n",
            "Epoch : 77 : w = 1.9550 : loss = 0.003113 \n",
            "Epoch : 79 : w = 1.9576 : loss = 0.002756 \n",
            "Epoch : 81 : w = 1.9601 : loss = 0.002441 \n",
            "Epoch : 83 : w = 1.9625 : loss = 0.002161 \n",
            "Epoch : 85 : w = 1.9647 : loss = 0.001914 \n",
            "Epoch : 87 : w = 1.9668 : loss = 0.001695 \n",
            "Epoch : 89 : w = 1.9687 : loss = 0.001501 \n",
            "Epoch : 91 : w = 1.9706 : loss = 0.001329 \n",
            "Epoch : 93 : w = 1.9723 : loss = 0.001177 \n",
            "Epoch : 95 : w = 1.9739 : loss = 0.001042 \n",
            "Epoch : 97 : w = 1.9755 : loss = 0.000923 \n",
            "Epoch : 99 : w = 1.9769 : loss = 0.000817 \n",
            "After Training f(5) : tensor([9.9539], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5l7GZBsyj3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}